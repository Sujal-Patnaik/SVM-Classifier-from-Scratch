{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeHwGEnzQ0x1nFNqLOFv85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sujal-Patnaik/SVM-Classifier-from-Scratch/blob/main/SVM(Classifier).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aiQEn_L1twSk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, kernel='linear', C=1.0, tol=1e-3, max_passes=5, max_iter=1000, degree=3, gamma=None):\n",
        "        self.C = C\n",
        "        self.tol = tol\n",
        "        self.max_passes = max_passes\n",
        "        self.max_iter = max_iter\n",
        "        self.kernel_type = kernel\n",
        "        self.degree = degree\n",
        "        self.gamma = gamma\n",
        "        self.b = 0.0\n",
        "\n",
        "    def kernel(self, x, y):\n",
        "        if self.kernel_type == 'linear':\n",
        "            return x @ y.T\n",
        "        elif self.kernel_type == 'polynomial':\n",
        "            return (x @ y.T + 1) ** self.degree\n",
        "        elif self.kernel_type == 'rbf':\n",
        "            if self.gamma is None:\n",
        "                self.gamma = 1 / x.shape[1]\n",
        "            x_norm = np.sum(x**2, axis=-1).reshape(-1, 1)\n",
        "            y_norm = np.sum(y**2, axis=-1).reshape(1, -1)\n",
        "            return np.exp(-self.gamma * (x_norm + y_norm - 2 * x @ y.T))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown kernel\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = y.astype(np.float64)\n",
        "        y[y == 0] = -1  # convert labels to {-1, 1}\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.alphas = np.zeros(n_samples)  #initialize the alphas as a 0 vector\n",
        "        self.b = 0.0 #initialize b to 0\n",
        "        self.K = self.kernel(X, X)\n",
        "\n",
        "        # Initialize the error cache\n",
        "        self.E_cache = self.project(self.X) - self.y\n",
        "\n",
        "        passes = 0\n",
        "        iters = 0\n",
        "\n",
        "        while passes < self.max_passes and iters < self.max_iter:\n",
        "            num_changed_alphas = 0\n",
        "            for i in range(n_samples):\n",
        "                # E_i from the error Cache\n",
        "                E_i = self.E_cache[i]\n",
        "\n",
        "                #KKT condition checking\n",
        "                cond1 = (y[i] * E_i < -self.tol) and (self.alphas[i] < self.C)\n",
        "                cond2 = (y[i] * E_i > self.tol) and (self.alphas[i] > 0)\n",
        "\n",
        "                #The i is chosen for which alpha[i] violates KKT(Karn Kush Tucker Conditions)\n",
        "                if cond1 or cond2:\n",
        "                    non_zero_alphas = np.where((self.alphas > 0) & (self.alphas < self.C))[0]\n",
        "                    if len(non_zero_alphas) > 1:\n",
        "                        # E_list contains the errors of all the non-zero alphas\n",
        "                        E_list = self.E_cache[non_zero_alphas]\n",
        "                        #j is chosen to maximize |E_i - E_j| if possible (i.e., from non-bound alphas).\n",
        "                        j = non_zero_alphas[np.argmax(np.abs(E_list - E_i))]\n",
        "                    else:\n",
        "                        #Otherwise, it is randomly selected.\n",
        "                        j = np.random.randint(0, n_samples)\n",
        "                        while j == i:\n",
        "                            j = np.random.randint(0, n_samples)\n",
        "\n",
        "                    # Extracting E_j from the Error Cache\n",
        "                    E_j = self.E_cache[j]\n",
        "\n",
        "                    alpha_i_old, alpha_j_old = self.alphas[i], self.alphas[j]\n",
        "\n",
        "                    #Finding the Lower and Upper Bounds to clip the alpha[j]\n",
        "                    if y[i] != y[j]:\n",
        "                        L = max(0, alpha_j_old - alpha_i_old)\n",
        "                        H = min(self.C, self.C + alpha_j_old - alpha_i_old)\n",
        "                    else:\n",
        "                        L = max(0, alpha_i_old + alpha_j_old - self.C)\n",
        "                        H = min(self.C, alpha_i_old + alpha_j_old)\n",
        "\n",
        "                    if L == H:\n",
        "                        continue\n",
        "\n",
        "                    eta = 2.0 * self.K[i, j] - self.K[i, i] - self.K[j, j]\n",
        "                    if eta >= 0:\n",
        "                        continue  #This ensures that we update the pair (i,j) only if the curvature of the objective function is suitable for descent\n",
        "\n",
        "                    self.alphas[j] -= y[j] * (E_i - E_j) / eta #updating the alpha[j]\n",
        "                    self.alphas[j] = np.clip(self.alphas[j], L, H)\n",
        "\n",
        "                    if abs(self.alphas[j] - alpha_j_old) < 1e-5:\n",
        "                        continue\n",
        "\n",
        "                    self.alphas[i] += y[i] * y[j] * (alpha_j_old - self.alphas[j]) #updating the alpha[i] after alpha[j] is found out\n",
        "\n",
        "                    b1 = self.b - E_i - y[i] * (self.alphas[i] - alpha_i_old) * self.K[i, i] - y[j] * (self.alphas[j] - alpha_j_old) * self.K[i, j]\n",
        "                    b2 = self.b - E_j - y[i] * (self.alphas[i] - alpha_i_old) * self.K[i, j] - y[j] * (self.alphas[j] - alpha_j_old) * self.K[j, j]\n",
        "\n",
        "                    if 0 < self.alphas[i] < self.C:\n",
        "                        self.b = b1    #This means alpha[i] is a support vector\n",
        "                    elif 0 < self.alphas[j] < self.C:\n",
        "                        self.b = b2    #This means alpha[j] is a support vector\n",
        "                    else:\n",
        "                        self.b = (b1 + b2) / 2  #Since neither alpha[i] nor alpha[j] are support vectors, b1 and b2 are averaged to get b\n",
        "\n",
        "                    # Updating the Error Cache\n",
        "                    delta_b = self.b - self.b_old if hasattr(self, 'b_old') else self.b\n",
        "                    self.E_cache += (y[i] * (self.alphas[i] - alpha_i_old) * self.K[:, i] +\n",
        "                                     y[j] * (self.alphas[j] - alpha_j_old) * self.K[:, j] + delta_b)\n",
        "                    self.b_old = self.b\n",
        "\n",
        "                    num_changed_alphas += 1\n",
        "\n",
        "            if num_changed_alphas == 0:\n",
        "                passes += 1\n",
        "            else:\n",
        "                passes = 0\n",
        "\n",
        "            iters += 1\n",
        "\n",
        "        self.support_ = np.where(self.alphas > 1e-5)[0]\n",
        "        self.alpha_sv = self.alphas[self.support_]\n",
        "        self.X_sv = X[self.support_]\n",
        "        self.y_sv = y[self.support_]\n",
        "        print(f\"Training complete (SMO). Support vectors found: {len(self.support_)}\")\n",
        "\n",
        "    def _decision_function_index(self, i):\n",
        "        return np.sum(self.alphas * self.y * self.K[i]) + self.b\n",
        "\n",
        "    def _decision_function(self, X):\n",
        "        return np.dot((self.alphas * self.y), self.kernel(X, self.X).T) + self.b\n",
        "\n",
        "    def project(self, X):\n",
        "        return self._decision_function(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(self.project(X))\n",
        "\n",
        "    def fit_gd(self, X, y, lr=0.001, epochs=1000):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = y.astype(np.float64)\n",
        "        y[y == 0] = -1\n",
        "\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            margin = y * (X @ self.w + self.b)\n",
        "            mask = margin < 1\n",
        "\n",
        "            dw = self.w - self.C * (X[mask].T @ y[mask])\n",
        "            db = -self.C * np.sum(y[mask])\n",
        "\n",
        "            self.w -= lr * dw\n",
        "            self.b -= lr * db\n",
        "\n",
        "        print(\"Training complete (vectorized gradient descent).\")\n",
        "        self.support_ = np.where(y * (X @ self.w + self.b) < 1)[0]\n",
        "        self.X_sv = X[self.support_]\n",
        "        self.y_sv = y[self.support_]\n",
        "\n",
        "    def predict_gd(self, X):\n",
        "        if not hasattr(self, 'w'):\n",
        "            raise AttributeError(\"Model has not been trained using gradient descent.\")\n",
        "        return np.sign(X @ self.w + self.b)\n",
        "\n",
        "    def project_gd(self, X):\n",
        "        return X @ self.w + self.b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "mask = (digits.target == 1) | (digits.target == 7)\n",
        "X = digits.data[mask, :10]  # First 10 features\n",
        "y = digits.target[mask]\n",
        "y = np.where(y == 1, 1, -1)  # 1 and -1\n"
      ],
      "metadata": {
        "id": "Vm9MnNCgtyak"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# scratch built SVM\n",
        "custom_svm = SVM(kernel='linear', C=1.0)\n",
        "custom_svm.fit(X_train, y_train)\n",
        "y_pred_custom = custom_svm.predict(X_test)\n",
        "\n",
        "# Scikit-learn's SVM\n",
        "from sklearn.svm import SVC\n",
        "sklearn_svm = SVC(kernel='linear', C=1.0)\n",
        "sklearn_svm.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_svm.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR_FeM4Wt7EQ",
        "outputId": "1208e519-ab34-4e8d-9b52-216392c42459"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete (SMO). Support vectors found: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Scratch SVM Accuracy:\", accuracy_score(y_test, y_pred_custom))\n",
        "print(\"Sklearn SVM Accuracy:\", accuracy_score(y_test, y_pred_sklearn))\n",
        "\n",
        "print(\"\\nScratch SVM Report:\\n\", classification_report(y_test, y_pred_custom))\n",
        "print(\"\\nSklearn SVM Report:\\n\", classification_report(y_test, y_pred_sklearn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCPn10wEuEW5",
        "outputId": "4527e257-b028-4b14-9370-0263bfad3849"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scratch SVM Accuracy: 0.8623853211009175\n",
            "Sklearn SVM Accuracy: 0.8715596330275229\n",
            "\n",
            "Scratch SVM Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.83      0.88      0.85        50\n",
            "           1       0.89      0.85      0.87        59\n",
            "\n",
            "    accuracy                           0.86       109\n",
            "   macro avg       0.86      0.86      0.86       109\n",
            "weighted avg       0.86      0.86      0.86       109\n",
            "\n",
            "\n",
            "Sklearn SVM Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.85      0.88      0.86        50\n",
            "           1       0.89      0.86      0.88        59\n",
            "\n",
            "    accuracy                           0.87       109\n",
            "   macro avg       0.87      0.87      0.87       109\n",
            "weighted avg       0.87      0.87      0.87       109\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "#Training the sklearn SVM\n",
        "clf = SVC(C=1.0, kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_sklearn = clf.predict(X_test)\n",
        "\n",
        "acc_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Sklearn SVM Accuracy: {acc_sklearn:.4f}\")\n",
        "\n",
        "#Training the SVM built from scratch\n",
        "my_svm = SVM(C=1.0)\n",
        "my_svm.fit(X_train, y_train)\n",
        "y_pred_custom = my_svm.predict(X_test)\n",
        "\n",
        "acc_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Scratch SVM Accuracy: {acc_custom:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDk1qq6WuIaZ",
        "outputId": "f2fe6be5-b5dd-4004-8a0f-02ec7c889ab8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn SVM Accuracy: 0.9561\n",
            "Training complete (SMO). Support vectors found: 36\n",
            "Scratch SVM Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below 2 cells I have used One Vs Rest approach for multiclass classification using my SVM from scratch. In this approach we use N binary classifiers for N number of classes and combine their results."
      ],
      "metadata": {
        "id": "mfLuh8StXgzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target  # classes: 0, 1, 2\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "custom_svms = []\n",
        "\n",
        "# Training one binary SVM per class\n",
        "for cls in classes:\n",
        "    y_binary = np.where(y_train == cls, 1, -1)\n",
        "    clf = SVM(kernel='rbf', C=1.0)\n",
        "    clf.fit(X_train, y_binary)\n",
        "    custom_svms.append(clf)\n",
        "\n",
        "# Predict using decision function from all classifiers\n",
        "custom_preds = []\n",
        "for x in X_test:\n",
        "    scores = [clf.project(x.reshape(1, -1))[0] for clf in custom_svms]\n",
        "    pred = np.argmax(scores)\n",
        "    custom_preds.append(pred)\n",
        "\n",
        "custom_acc = accuracy_score(y_test, custom_preds)\n",
        "print(f\"\\nScratch SVM (OvR) Accuracy: {custom_acc:.4f}\")\n",
        "\n",
        "# ========== Sklearn SVC ==========\n",
        "sklearn_clf = SVC(kernel='rbf')\n",
        "sklearn_clf.fit(X_train, y_train)\n",
        "sklearn_preds = sklearn_clf.predict(X_test)\n",
        "sklearn_acc = accuracy_score(y_test, sklearn_preds)\n",
        "print(f\"Sklearn SVC Accuracy: {sklearn_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_q2TYFFuOn4",
        "outputId": "542c4f32-e9d1-46ea-9af5-f98e690a0307"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete (SMO). Support vectors found: 12\n",
            "Training complete (SMO). Support vectors found: 32\n",
            "Training complete (SMO). Support vectors found: 31\n",
            "\n",
            "Scratch SVM (OvR) Accuracy: 1.0000\n",
            "Sklearn SVC Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Loading the digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train custom SVMs with One-vs-Rest (OvR)\n",
        "classes = np.unique(y_train)\n",
        "custom_svms = []\n",
        "\n",
        "for cls in classes:\n",
        "    # Create binary labels: 1 for current class, -1 for others\n",
        "    y_binary = np.where(y_train == cls, 1, -1)\n",
        "\n",
        "    # Train a custom SVM for each class\n",
        "    clf = SVM(kernel='rbf', C=10.0, gamma=0.001)\n",
        "    clf.fit(X_train, y_binary)\n",
        "    custom_svms.append(clf)\n",
        "\n",
        "# Make predictions using the OvR(One Vs Rest) strategy\n",
        "custom_preds = []\n",
        "for x in X_test:\n",
        "    # Get scores from all 10 classifiers\n",
        "    scores = [clf.project(x.reshape(1, -1))[0] for clf in custom_svms]\n",
        "\n",
        "    # Predict the class with the highest score\n",
        "    pred = np.argmax(scores)\n",
        "    custom_preds.append(pred)\n",
        "\n",
        "# Checking the scratch SVM's accuracy\n",
        "custom_acc = accuracy_score(y_test, custom_preds)\n",
        "print(f\"\\nScratch SVM (OvR with SMO) Accuracy on Digits: {custom_acc:.4f}\")\n",
        "\n",
        "## skicit learn SVM accuracy\n",
        "# Using the same hyperparameters for a fair comparison\n",
        "sklearn_clf = SVC(kernel='rbf', C=10.0, gamma=0.001)\n",
        "sklearn_clf.fit(X_train, y_train)\n",
        "sklearn_preds = sklearn_clf.predict(X_test)\n",
        "sklearn_acc = accuracy_score(y_test, sklearn_preds)\n",
        "print(f\"Sklearn SVC Accuracy on Digits: {sklearn_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_cebcuRvUYI",
        "outputId": "fdc516a9-7858-43ff-bad4-77ea38cf3e33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete (SMO). Support vectors found: 81\n",
            "Training complete (SMO). Support vectors found: 149\n",
            "Training complete (SMO). Support vectors found: 169\n",
            "Training complete (SMO). Support vectors found: 156\n",
            "Training complete (SMO). Support vectors found: 146\n",
            "Training complete (SMO). Support vectors found: 146\n",
            "Training complete (SMO). Support vectors found: 111\n",
            "Training complete (SMO). Support vectors found: 141\n",
            "Training complete (SMO). Support vectors found: 226\n",
            "Training complete (SMO). Support vectors found: 181\n",
            "\n",
            "Scratch SVM (OvR with SMO) Accuracy on Digits: 0.9889\n",
            "Sklearn SVC Accuracy on Digits: 0.9907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1EFG2blrhOo"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}